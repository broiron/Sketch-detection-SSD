{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2ce54c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/broiron/.local/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from data import *\n",
    "from PIL import Image\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.functions import Detect\n",
    "\n",
    "\n",
    "import torch.utils.data as data\n",
    "from ssd import build_ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc2f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_net(save_folder, net, cuda, testset, transform, thresh):\n",
    "    # dump predictions and assoc. ground truth to text file for now\n",
    "    filename = os.path.join(save_folder, 'test1.txt')\n",
    "    num_images = len(testset)\n",
    "    for i in range(num_images):\n",
    "        print('Testing image {:d}/{:d}....'.format(i+1, num_images))\n",
    "        img, annotation, _, _, img_id = testset.pull_item(i)\n",
    "        print(img_id)\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        x = img.unsqueeze(0)\n",
    "        x = x.type(torch.FloatTensor)\n",
    "        x = x.to(device)\n",
    "        x = Variable(x)\n",
    "\n",
    "        with open(filename, mode='a') as f:\n",
    "            f.write('\\nGROUND TRUTH FOR: '+img_id+'\\n')\n",
    "            for box in annotation:\n",
    "                f.write('label: '+' || '.join(str(b) for b in box)+'\\n')\n",
    "\n",
    "        net = net.to(device)\n",
    "\n",
    "        y = net(x)      # forward pass\n",
    "        detections = y.data\n",
    "        # scale each detection back up to the image\n",
    "        scale = torch.Tensor([img.shape[1], img.shape[0],\n",
    "                             img.shape[1], img.shape[0]])\n",
    "        pred_num = 0\n",
    "        for i in range(detections.size(1)):\n",
    "            j = 0\n",
    "            while detections[0, i, j, 0] >= args.visual_threshold:\n",
    "                if pred_num == 0:\n",
    "                    with open(filename, mode='a') as f:\n",
    "                        f.write('PREDICTIONS: '+'\\n')\n",
    "                score = detections[0, i, j, 0]\n",
    "                label_name = CUSTOM_CLASSES[i-1]\n",
    "                pt = (detections[0, i, j, 1:]*scale).cpu().numpy()\n",
    "                coords = (pt[0], pt[1], pt[2], pt[3])\n",
    "                pred_num += 1\n",
    "                with open(filename, mode='a') as f:\n",
    "                    f.write(str(pred_num)+' label: '+label_name+' score: ' +\n",
    "                            str(score) + ' '+' || '.join(str(c) for c in coords) + '\\n')\n",
    "                print(str(pred_num)+' label: '+label_name+' score: ' +\n",
    "                            str(score) + ' '+' || '.join(str(c) for c in coords))\n",
    "                j += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b24ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect = Detect(num_classes=2, bkg_label=1, top_k=200, conf_thresh=0.01, nms_thresh=0.445)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38df1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a3cb554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "dataset = COCODetection(root='/home/broiron/broiron/line_dataset_vol1_coco/', \n",
    "                        transform=SSDAugmentation(300, MEANS), image_set='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0db41eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/broiron/broiron/model_train/ssd-pytorch-custom/utils/augmentations.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n"
     ]
    }
   ],
   "source": [
    "img, an, _, _ = dataset.pull_item(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea61a5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9fd73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting with default value\n",
    "# optimizer = optim.SGD(net.parameters(), lr=1e-5, momentum=0.9, # 1e-3 -> 1e-5\n",
    "#                         weight_decay=5e-4) \n",
    "'''\n",
    "criterion = MultiBoxLoss(num_classes=2, overlap_thresh=0.5, prior_for_matching=True, \n",
    "                         bkg_label=1, neg_mining=True, neg_pos=3, neg_overlap=0.5, \n",
    "                         encode_target=False, use_gpu=True)\n",
    "'''\n",
    "\n",
    "criterion = MultiBoxLoss(2, 0.5, True, 0, True, 3, 0.5, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cd99105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_collate(batch):\n",
    "    \"\"\"Custom collate fn for dealing with batches of images that have a different\n",
    "    number of associated object annotations (bounding boxes).\n",
    "    Arguments:\n",
    "        batch: (tuple) A tuple of tensor images and lists of annotations\n",
    "    Return:\n",
    "        A tuple containing:\n",
    "            1) (tensor) batch of images stacked on their 0 dim\n",
    "            2) (list of tensors) annotations for a given image are stacked on 0 dim\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    imgs = []\n",
    "    for sample in batch:\n",
    "        imgs.append(sample[0])\n",
    "        targets.append(torch.FloatTensor(sample[1]))\n",
    "    return torch.stack(imgs, 0), targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1e47308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model!\n",
      "loading dataset...\n",
      "Testing SSD on:  MS COCO\n",
      "Data loader length... 100\n"
     ]
    }
   ],
   "source": [
    "net = build_ssd(phase='test', size=300, num_classes=2)\n",
    "net.load_state_dict(torch.load('./weights/linedataset_vol1_1a.pth'))\n",
    "net.eval()\n",
    "print(\"Finished loading model!\")\n",
    "\n",
    "loc_loss = 0\n",
    "conf_loss = 0\n",
    "epoch = 0\n",
    "batch_size = 1\n",
    "\n",
    "print('loading dataset...')\n",
    "\n",
    "print('Testing SSD on: ', dataset.name)\n",
    "\n",
    "step_index = 0\n",
    "\n",
    "data_loader = data.DataLoader(dataset, batch_size, num_workers=4, shuffle=False, collate_fn=detection_collate,\n",
    "                             pin_memory=True)\n",
    "print('Data loader length...', len(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49722b34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/broiron/broiron/model_train/ssd-pytorch-custom/utils/augmentations.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/home/broiron/broiron/model_train/ssd-pytorch-custom/utils/augmentations.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/home/broiron/broiron/model_train/ssd-pytorch-custom/utils/augmentations.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/home/broiron/broiron/model_train/ssd-pytorch-custom/utils/augmentations.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n",
      "error occured in iterator\n",
      "torch.Size([1, 8732, 2])\n"
     ]
    }
   ],
   "source": [
    "batch_iterator = iter(data_loader)\n",
    "for iteration in range(0, len(data_loader)): # 1개 sample 순회\n",
    "    try:\n",
    "        images, targets = next(batch_iterator)\n",
    "    except:\n",
    "        print('error occured in iterator')\n",
    "    with torch.no_grad():\n",
    "        images = Variable(images.to(device))\n",
    "        targets = [Variable(ann.to(device)) for ann in targets]\n",
    "        out = net.forward(images) # forward pass   \n",
    "        out = detect.forward(out[0], out[1], out[2])\n",
    "        \n",
    "    detections = out.data\n",
    "    scale = torch.Tensor(im)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe61c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssd_train",
   "language": "python",
   "name": "ssd_train"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
